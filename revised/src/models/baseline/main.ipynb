{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import scipy.stats\n",
    "import nltk\n",
    "from nltk.tag import pos_tag \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "DATA_DIR = \"../../../data/\"\n",
    "OUTPUT_DIR = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughout the rest of the code, it is assumed that the keys of the following dictionary\n",
    "# are global variables, available to freely use. These global variables are modified only in this block. \n",
    "json_files = {'abstract_sentences': \"abstracts.json\", \n",
    "                'full_text_sentences': \"fulltexts.json\", \n",
    "                'labels': \"labels.json\",\n",
    "                'data_splits': \"data_splits.json\"}   ## New Preprocess\n",
    "              #'train_indices': 'train_indices.json',\n",
    "                #'test_indices': 'test_indices.json'}\n",
    "\n",
    "for varname, j in json_files.items():\n",
    "    qualified_name = DATA_DIR + j\n",
    "    file = open(qualified_name, \"rb\")\n",
    "    exec(varname + \" = json.load(file)\")\n",
    "    \n",
    "relevant_file_numbers = [x[0] for x in labels]\n",
    "\n",
    "## New Preprocess\n",
    "#full_text_sentences = np.array(full_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_word(word):\n",
    "    return word not in eng_stopwords and word != ',' and word != '.' and word != '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wc_map(full_text_sentences):\n",
    "    word_counts_map = []\n",
    "    for document in full_text_sentences.values():\n",
    "    #for document in full_text_sentences: #Modified\n",
    "        cnt = Counter()\n",
    "        for sentence in document:\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if keep_word(word):\n",
    "                    cnt[word] += 1\n",
    "            word_counts_map.append(cnt)\n",
    "    return word_counts_map\n",
    "\n",
    "word_counts_map = get_wc_map(full_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_scores(full_text_sentences, word_counts_map):\n",
    "    document_sentence_scores = {} # Map of maps from sentence_id to score.\n",
    "    #document_sentence_scores = [] # List of maps from sentence_id to score. #Modified\n",
    "    \n",
    "    for i,(document_id, document) in enumerate(full_text_sentences.items()): #Modified\n",
    "        sentence_scores = {} # Map for this document.\n",
    "        document_word_counts = word_counts_map[i] \n",
    "        num_doc_words = sum(document_word_counts.values())\n",
    "\n",
    "        for sentence_id, sentence in enumerate(document):\n",
    "            sentence_word_freq_sum = 0\n",
    "            num_words_in_sentence = 0\n",
    "\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if keep_word(word):\n",
    "                    word_freq = document_word_counts[word] / num_doc_words\n",
    "                    sentence_word_freq_sum += word_freq\n",
    "                    num_words_in_sentence += 1\n",
    "\n",
    "            sentence_score = sentence_word_freq_sum / num_words_in_sentence if num_words_in_sentence != 0 else 0\n",
    "            sentence_scores[sentence_id] = sentence_score \n",
    "        document_sentence_scores[document_id] = sentence_scores\n",
    "        #document_sentence_scores.append(sentence_scores) #Modified\n",
    "    return document_sentence_scores\n",
    "\n",
    "document_sentence_scores = get_doc_scores(full_text_sentences, word_counts_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_position(document_num, sentence_num):\n",
    "    return sentence_num / len(full_text_sentences[document_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(document_num, sentence_num, mean_sent_length=None, std_dev=5):\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_noun(document_num, sentence_num):\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    tagged_sent = pos_tag(sentence)\n",
    "    propernouns = [word for word, pos in tagged_sent if pos == 'NNP']    \n",
    "    return len(propernouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_freq_score(document_num, sentence_num):\n",
    "    score = 1000 * document_sentence_scores[document_num][sentence_num]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_functions = [sentence_position, sentence_length, proper_noun, sentence_freq_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15022"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices) + len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_numbers = [x[0] for x in labels]\n",
    "\n",
    "## New Preprocess\n",
    "train_indices = data_splits['train']\n",
    "test_indices = data_splits['test']\n",
    "\n",
    "## SUBSET TESTING\n",
    "#import math\n",
    "#train_small = train_indices[:math.floor(0.1*len(train_indices))]\n",
    "#test_small = train_indices[:math.floor(0.1*len(test_indices))]\n",
    "\n",
    "# Make sure these are regular lists and not numpy arrays. Things will break if they are numpy arrays.\n",
    "assert type(train_indices) == list\n",
    "assert type(test_indices) == list\n",
    "\n",
    "# print(train_indices)\n",
    "# print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "def create_ft_matrix(test=True, number=10):\n",
    "    # Creates an X matrix with the train index file numbers' sentences appearing FIRST<\n",
    "    # followed by test index file numbers' sentences.\n",
    "    X = np.zeros((1, 4))\n",
    "    ## SUBSET TESTING:\n",
    "    #for i in train_small + test_small:\n",
    "    for i in train_indices + test_indices:\n",
    "        document = full_text_sentences[i]\n",
    "        for j, sentence in enumerate(document):\n",
    "            X = np.vstack([X, [function(i, j) for function in features_functions]])\n",
    "    X = X[1:]\n",
    "    return X\n",
    "\n",
    "def get_corr_labels(test=True, number=10):\n",
    "    ## SUBSET TESTING:\n",
    "    #relevant_file_numbers = train_small + test_small\n",
    "    relevant_file_numbers = train_indices + test_indices\n",
    "    corr_labels = []\n",
    "    \n",
    "    for file_num, labels_list in labels.items(): #Modified\n",
    "        if file_num in relevant_file_numbers:\n",
    "            corr_labels.append(labels_list)\n",
    "    return corr_labels\n",
    "\n",
    "def get_num_sentences(file_numbers):\n",
    "    total_num_sentences = 0\n",
    "    for i in file_numbers:\n",
    "        sentences = full_text_sentences[i]\n",
    "        total_num_sentences += len(sentences) \n",
    "    return total_num_sentences\n",
    "        \n",
    "\n",
    "def flatten(lst):\n",
    "    flattened_list = []\n",
    "    for sublist in lst:\n",
    "        for item in sublist:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_ft_matrix() #12:45-12:48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1335\n"
     ]
    }
   ],
   "source": [
    "corr_labels = flatten(get_corr_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " ('nyt_2007_03_03_1830052',\n",
       "  [1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(labels.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51936"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(v) for k,v in labels.items() if k in train_small])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1335\n",
      "167\n",
      "51936\n"
     ]
    }
   ],
   "source": [
    "print(len(train_small))\n",
    "print(len(test_small))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58829, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4415b7961500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert X.shape[0] == len(corr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [58829, 51936]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8de7c872e5b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.15\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [58829, 51936]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Number of sentences (i.e. rows) to use for training.\n",
    "nts = get_num_sentences(train_indices)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[0:nts], X[nts:], y[0:nts], y[nts:]\n",
    "\n",
    "threshold = 0.5\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "predictions = [1 if x[1] > 0.15 else 0 for x in probabilities]\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(probabilities) == get_num_sentences(test_indices) # Must have a probability for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(probabilities):\n",
    "    output = dict()\n",
    "    \n",
    "    used_so_far = 0\n",
    "    for test_index in test_indices:\n",
    "        doc = full_text_sentences[test_index]\n",
    "        ns = len(doc) # number of sentences in this document\n",
    "        \n",
    "        output[test_index] = probabilities[used_so_far : used_so_far + ns].tolist()\n",
    "        used_so_far += ns\n",
    "    return output\n",
    "\n",
    "output = generate_output(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(output):\n",
    "    json.dump(output, open(OUTPUT_DIR + \"baseline_probabilities.json\", 'w'))\n",
    "\n",
    "save_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
