Using the validation dataset the optimal threshold which maximizes
Rouge scores is determined.  Then on the test dataset Rouge Scores
are calcuated with this threshold and compared to the Rouge Scores
of the ground truth labels.  A ROC curve is generated by trying all
thresholds between 0 and 1 with intervals of 0.01.

To use run evaluate_opt.py <data folder> <model folder> [<threshold value>]
(The threshold value is optional.  If provided the evaluation will skip
optimizing the threshold with the validation dataset)

The script assumes <model folder> has an outputs folder which contains a file
named predictions.json.  This file should be a dictionary mapping document ids
from both the validation and test dataset to a list of probability predictions.

<data folder> should contain abstracts.json, sentence_tokens.json, data_splits.json,
and labels.json.

In addition to printing out statistics about scores, running evaluate_opt.py 
will add files for each model to Plots (for various metrics)

-ROC_Curves contain information about ROC curves

-NP_Analysis contains analysis regarding breakdown by the 
number of sentences predicted by label

-DLB_Analysis contains analysis regarding breakdown by the
length of the documented being summarized

To compare different models on these metrics use compare.py