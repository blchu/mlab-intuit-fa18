Using the validation dataset the optimal threshold which maximizes
Rouge scores is determined.  Then on the test dataset Rouge Scores
are calcuated with this threshold and compared to the Rouge Scores
of the ground truth labels.  A ROC curve is generated by trying all
thresholds between 0 and 1 with intervals of 0.01.

To use run evaluate_opt.py <data folder> <model folder> [<threshold value>]
(The threshold value is optional.  If provided the evaluation will skip
optimizing the threshold with the validation dataset)

The script assumes <model folder> has an outputs folder which contains a file
named predictions.json.  This file should be a dictionary mapping document ids
from both the validation and test dataset to a list of probability predictions.

<data folder> should contain abstracts.json, sentence_tokens.json, data_splits.json,
and labels.json.