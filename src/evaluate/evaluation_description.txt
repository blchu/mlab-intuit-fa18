Using the validation dataset the optimal threshold which maximizes
Rouge scores is determined.  Then on the test dataset Rouge Scores
are calcuated with this threshold and compared to the Rouge Scores
of the ground truth labels.  A ROC curve is generated by trying all
thresholds between 0 and 1 with intervals of 0.01.

To use run evaluate_opt.py <model folder> <data folder>
The terminal will prompt you input whether or not the model is extractive or abstractive.

If the model is extractive the expectation is that within <model folder> there is a
folder 'outputs' which contains a file 'predictions.json'.  This json files contains
a dictionary mapping document ids to lists of probability predictions (probability a
sentence is included in the summary.)  If threshold optimization is required (more on
this later) the dictionary must contain mappings from all doc ids in both the 
validation and test dataset.  If threshold optimization is not required the dictionary
must map from all doc ids in the test dataset.  Note excess mappings do not affect
the evaluation script.

If the model is abstractive the expectation remains that within <model folder> there is 
a folder 'outputs' which contains a file 'predictions.json'.  However now this json file
should contain a dictionary mapping document ids to strings which represent the
predicted summary.  The dictionary must contain mappings from all doc ids in the test dataset.

<data folder> should contain abstracts.json, sentence_tokens.json, data_splits.json,
and labels.json.

For extractive model, the binary thresholds which separates probabilities into 0s and 1s
is integral to proper evaluation.  For this reason the script will prompt you as to
whether you wish to conduct threshold optimization.  Threshold optimization is scanning
various thresholds and selecting the one that performs best on the validation or a subset
of the validation dataset.  The script also allows you to provide a threshold if you would
like to skip this step.

The primary metric provided are ROUGE scores broken down by the different scores
and their respective precisions and recalls.  Additionally plots will be displayed
(DLB analysis for both, additional ROC curves and NP_Analysis for extractive).

In addition to displaying statistics about scores, running evaluate_opt.py 
will add files for each model to Plots (for various metrics)

-ROC_Curves contain information about ROC curves (Only for extractive models)

-NP_Analysis contains analysis regarding breakdown by the 
number of sentences predicted by label (Only for extractive models)

-DLB_Analysis contains analysis regarding breakdown by the
length of the documented being summarized

To compare different models on these metrics use compare.py