Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond: https://arxiv.org/abs/1602.06023
-In abstractive summarization, we are essentially mapping an input sequence of words into an output sequence that might have different vocabulary, the output should be a concise summary of the input (less than half the size, in terms of # words)
-Sequence to sequence RNN architectures are naturally suited for this task: deep learning models can learn the necessary parameters needed to perform this highly complex summarization; currently, these models are also used for speech recognition, video captioning, translation, etc.
-However, with abstractive text summarization, the output sequence is much smaller than input sequence so cannot use same architecture/approach as say for machine translation (no one to one relationship)
-Turns out, you can use the same attention-based seq to seq RNN used for machine translation to outperform the state of the art in abstractive summarization, so it still works; can tweak these models to achieve even better performance
-Note: they also proposed a new dataset for abstractive text summarization of a document into a few sentences
-RNN architecture: encoder consists of a bidirectional GRU-RNN, while the decoder consists of a uni-directional GRU-RNN with the same hidden-state size as that of the encoder, and an attention mechanism over the source-hidden states and a softmax layer over target vocabulary to generate words; idea: you "encode" the document and then "decode" it to output the summarization
-Modification: decoder-vocabulary of each mini-batch is restricted to words in the source documents of that batch + the most frequent words in the target dictionary are added until the vocabulary reaches a fixed size (focuses the algorithm on the words most essential to the summary)
-In addition to word embeddings, we need to capture parts-of-speech tags, named-entity tags, and TF and IDF statistics of the words in order to create a good model
-Avoid using the "unk" token since it isn't good for the summary, so instead if we want to use a rare word in the summary (not in our dictionary), simply use the same word as was used in the source document
-Hierarchical attention: in a large document, is important to identify the key sentences from which the summary should be drawn (kind of like in extractive but using attention-based models); so we use 2 bidirectional RNNs, one at the sentence level and one at the word level; the attention model works at each level, and word-level attention is reweighed using the sentence-level attention

A Deep Reinforced Model for Abstractive Summarization: https://arxiv.org/pdf/1705.04304.pdf
-Problem: attention-based RNN models for abstractive summarization achieve good results on small paragraphs, but for larger documents they fail and often produce repetitive sentences
-Propose new method of combining standardized supervised learning method with RL to attend over the input and continuously generated output separately; eliminate exposure bias because these don't assume ground truth is provided
-Achieved state of the art results on CNN and NYTimes datasets
-Attention-based model is novel:  intra-temporal attention in the encoder that records previous attention weights for each of the input tokens while a sequential intra-attention model in the decoder takes into account which words have already been generated by the decoder
-Basically, the idea is to prevent the model from looking over and repeating the same thing over and over again
-Their encoder makes no assumption about the decoder RNN, making it more widely applicable than previous studies
-To generate a token, our decoder uses either a token-generation softmax layer or a pointer mechanism to copy rare or unseen words from the input sequence
-We share the same embedding matrix between the encoder and decoder sequences as well as with the token generation layer
-Training the model: teacher forcing algorithm basically minimizes a maximum-likelihood loss at each decoding step, but doesn't always produce the best results (seen with image captioning and machine translation applications)
-Instead of trying to minimize this metric, instead learn how to optimize a discrete policy using reinforcement learning: can use the self-critical policy gradient training algorithm, defining a reward function using whatever evaluation metric

Extractive text summarization system to aid data extraction from full text in systematic review development: https://www.sciencedirect.com/science/article/pii/S1532046416301514
-Developed extractive algorithm to create summaries of full-text scientific publications; compared to human summaries, computer summaries covered a greater depth of the information and had higher density of relevant sentences
-Goal: classify sentences that contain PICO (Population, Intervention, Control, and Outcome) elements, a way to formulate answers to questions in the medical field; also perform meta-analysis on large datasets
-ExaCT: state of the art in summarizing clinical papers, uses an ML model to extract the top 5 relevant sentences for each of the PICO elements (20 sentences in all); trained on papers in the top 5 clinical journals, but requires some hand engineering
-This paper develops fully automatic extractive text summarization for scientific articles
-Started with a gold standard for summarization, namely summaries generated by hand by humans currently in the Cochrane archive; papers were mainly about heart and circulation research; did processing to identify keywords of each article, study arms, population studied, etc.
-Workflow: send the paper through our text summarization algorithm, get the summary, compare/evaluate with the gold standard
-9 stages of summarization: 1) PDF text extraction; 2) Text Classification & Filtering, classifying text as Body, Title, Abstract, Metadata, etc.; 3) Text Normalization, translate text into canonical form, so one hundred and three becomes 103; 4) IMRAD Context Detection, basically trying to assign each text snippet to one of the categories of a scientific paper (introduction, methods, results, analysis, discussion); 5) 
Sentence Segmentation, essentially splitting up sentences into different context nodes; 6) Sentence Filtering, filter out sentences that are irrelevant for summary (discuss background knowledge, are introductory, etc.); 7) Sentence Ranking, essentially prioritizing sentences for each individual data element, uses SVM regression with polynomial kernel, bag of terms, context group, and semantic group; 8) Key phrase Extraction, basically recognizing key phrases within sentences; 9) Post-processing, filtering lengthy phrases, phrases within other phrases, etc.; 10) Evaluation: compare the computer-generated summarization with the gold standard, using precision and recall measurements
