{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import scipy.stats\n",
    "import nltk\n",
    "from nltk.tag import pos_tag \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "DATA_DIR = \"../../../../data/\"\n",
    "OUTPUT_DIR = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughout the rest of the code, it is assumed that the keys of the following dictionary\n",
    "# are global variables, available to freely use. These global variables are modified only in this block. \n",
    "json_files = {'abstract_sentences': \"AbstractSentences2007.json\", \n",
    "                'full_text_sentences': \"FullTextSentences2007.json\", \n",
    "                'labels': \"labels.json\",\n",
    "                'train_indices': 'train_indices.json',\n",
    "                'test_indices': 'test_indices.json'}\n",
    "\n",
    "for varname, j in json_files.items():\n",
    "    qualified_name = DATA_DIR + j\n",
    "    file = open(qualified_name, \"rb\")\n",
    "    exec(varname + \" = json.load(file)\")\n",
    "    \n",
    "relevant_file_numbers = [x[0] for x in labels]\n",
    "\n",
    "full_text_sentences = np.array(full_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_word(word):\n",
    "    return word not in eng_stopwords and word != ',' and word != '.' and word != '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wc_map(full_text_sentences):\n",
    "    word_counts_map = []\n",
    "    for document in full_text_sentences:\n",
    "        cnt = Counter()\n",
    "        for sentence in document:\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if keep_word(word):\n",
    "                    cnt[word] += 1\n",
    "            word_counts_map.append(cnt)\n",
    "    return word_counts_map\n",
    "\n",
    "word_counts_map = get_wc_map(full_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_scores(full_text_sentences, word_counts_map):\n",
    "    document_sentence_scores = [] # List of maps from sentence_id to score.\n",
    "    \n",
    "    for i, document in enumerate(full_text_sentences):\n",
    "        sentence_scores = {} # Map for this document.\n",
    "        document_word_counts = word_counts_map[i] \n",
    "        num_doc_words = sum(document_word_counts.values())\n",
    "\n",
    "        for sentence_id, sentence in enumerate(document):\n",
    "            sentence_word_freq_sum = 0\n",
    "            num_words_in_sentence = 0\n",
    "\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if keep_word(word):\n",
    "                    word_freq = document_word_counts[word] / num_doc_words\n",
    "                    sentence_word_freq_sum += word_freq\n",
    "                    num_words_in_sentence += 1\n",
    "\n",
    "            sentence_score = sentence_word_freq_sum / num_words_in_sentence if num_words_in_sentence != 0 else 0\n",
    "            sentence_scores[sentence_id] = sentence_score \n",
    "\n",
    "        document_sentence_scores.append(sentence_scores)\n",
    "    return document_sentence_scores\n",
    "\n",
    "document_sentence_scores = get_doc_scores(full_text_sentences, word_counts_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_position(document_num, sentence_num):\n",
    "    return sentence_num / len(full_text_sentences[document_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_length(document_num, sentence_num, mean_sent_length=None, std_dev=5):\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proper_noun(document_num, sentence_num):\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    tagged_sent = pos_tag(sentence)\n",
    "    propernouns = [word for word, pos in tagged_sent if pos == 'NNP']    \n",
    "    return len(propernouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_freq_score(document_num, sentence_num):\n",
    "    score = 1000 * document_sentence_scores[document_num][sentence_num]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_functions = [sentence_position, sentence_length, proper_noun, sentence_freq_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: 0.0002698327037236913, 2: 0.00030581039755351685, 3: 0.0012232415902140672, 4: 0.001146788990825688, 5: 0.0007312857332801489, 6: 0.00012742099898063202, 7: 0.0008737439930100481, 8: 0.003475118154017237, 9: 0.0005096839959225281, 10: 6.371049949031601e-05, 11: 0.00027800945232137893, 12: 0.0019113149847094803, 13: 0.0, 14: 0.00019113149847094801, 15: 0.0001529051987767584, 16: 0.0, 17: 0.0, 18: 0.0012592192840438928, 19: 0.0, 20: 0.0004704775346977182, 21: 0.00016989466530750936, 22: 0.0005096839959225281, 23: 0.0015290519877675841, 24: 0.00191131498470948, 25: 0.0011467889908256881, 26: 0.0015290519877675841, 27: 0.002378525314305131, 28: 0.00038226299694189603, 29: 0.00218435998252512, 30: 0.0, 31: 0.0, 32: 0.00021843599825251202, 33: 0.0, 34: 0.0006689602446483181, 35: 0.0024464831804281344, 36: 0.0007645259938837921, 37: 0.0011761938367442955, 38: 0.0012742099898063201, 39: 0.0, 40: 9.556574923547401e-05, 41: 0.0061162079510703364, 42: 0.0, 43: 0.0, 44: 0.00019113149847094801, 45: 0.0013900472616068945, 46: 0.0054608999563128, 47: 0.0, 48: 0.0007645259938837921, 49: 0.002102446483180428, 50: 0.0022935779816513763, 51: 0.0, 52: 0.001529051987767584, 53: 0.0006116207951070336, 54: 0.0015290519877675841, 55: 0.0011467889908256881, 56: 0.0013591573224600749, 57: 0.004757050628610262, 58: 0, 59: 0.0, 60: 0.0, 61: 0.00034526980368945444, 62: 0.0001834862385321101, 63: 0.00045871559633027525, 64: 0.00021843599825251202}\n"
     ]
    }
   ],
   "source": [
    "print(document_sentence_scores[28])\n",
    "\n",
    "file_numbers = [x[0] for x in labels]\n",
    "\n",
    "# Make sure these are regular lists and not numpy arrays. Things will break if they are numpy arrays.\n",
    "assert type(train_indices) == list\n",
    "assert type(test_indices) == list\n",
    "\n",
    "# print(train_indices)\n",
    "# print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "def create_ft_matrix(test=True, number=10):\n",
    "    # Creates an X matrix with the train index file numbers' sentences appearing FIRST<\n",
    "    # followed by test index file numbers' sentences.\n",
    "    X = np.zeros((1, 4))\n",
    "    \n",
    "    for i in train_indices + test_indices:\n",
    "        document = full_text_sentences[i]\n",
    "        for j, sentence in enumerate(document):\n",
    "            X = np.vstack([X, [function(i, j) for function in features_functions]])\n",
    "    X = X[1:]\n",
    "    return X\n",
    "\n",
    "def get_corr_labels(test=True, number=10):\n",
    "    relevant_file_numbers = train_indices + test_indices\n",
    "    corr_labels = []\n",
    "    \n",
    "    for file_num, labels_list in labels:\n",
    "        if file_num in relevant_file_numbers:\n",
    "            corr_labels.append(labels_list)\n",
    "    \n",
    "    return corr_labels\n",
    "\n",
    "def get_num_sentences(file_numbers):\n",
    "    total_num_sentences = 0\n",
    "    for i in file_numbers:\n",
    "        sentences = full_text_sentences[i]\n",
    "        total_num_sentences += len(sentences) \n",
    "    return total_num_sentences\n",
    "        \n",
    "\n",
    "def flatten(lst):\n",
    "    flattened_list = []\n",
    "    for sublist in lst:\n",
    "        for item in sublist:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_ft_matrix()\n",
    "corr_labels = flatten(get_corr_labels())\n",
    "y = corr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1352, 4)\n"
     ]
    }
   ],
   "source": [
    "assert X.shape[0] == len(corr_labels)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8238636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Number of sentences (i.e. rows) to use for training.\n",
    "nts = get_num_sentences(train_indices)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[0:nts], X[nts:], y[0:nts], y[nts:]\n",
    "\n",
    "threshold = 0.5\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "predictions = [1 if x[1] > 0.15 else 0 for x in probabilities]\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648, 4) (704, 4) 648 704\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(probabilities) == get_num_sentences(test_indices) # Must have a probability for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55162852, 0.44837148])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(probabilities):\n",
    "    output = dict()\n",
    "    \n",
    "    used_so_far = 0\n",
    "    for test_index in test_indices:\n",
    "        doc = full_text_sentences[test_index]\n",
    "        ns = len(doc) # number of sentences in this document\n",
    "        \n",
    "        output[test_index] = probabilities[used_so_far : used_so_far + ns].tolist()\n",
    "        used_so_far += ns\n",
    "    return output\n",
    "\n",
    "output = generate_output(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(output):\n",
    "    json.dump(output, open(OUTPUT_DIR + \"baseline_probabilities.json\", 'w'))\n",
    "\n",
    "save_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
