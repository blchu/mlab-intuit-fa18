SummaRuNNer Model Overview

This is a comprehensive overview of the SummaRuNNer custom implementation from this project.  
The research paper describing this model can be found here: https://arxiv.org/pdf/1611.04230.pdf
The overview is organized in the following sections:

(1) What Data the Model Needs
(2) How the Model Works
(3) Technical Notes and Variations
(4) How to Use the Model
(5) How to Tune the Model
(6) Understanding the Model's Performance

-----(1) What Data the Model Needs-----

For information regarding general data files see data/data_specifications.rtf

Training the model requires fulltexts.json, data_splits.json, and labels.json; however
the other two data files (abstract.json and sentence_tokens.json) are used for realtime
evaluation on the validation dataset.

Testing the model requires fulltexts.json and data_splits.json.

Additionally some model specific files are required.  These are generated by the
python script file_generator.py inside the SummaRuNNer folder and saved in a local folder
titled model_specific_files.  These files contain the word vector map trained on the data,
and the sentence cap which will be discussed in ‘How the Model Works’.  In order to
generate the model specific files run file_generator.py <data folder>.

-----(2) How the Model Works-----

SummaRuNNer is an extractive model which aims to select sentences from a document
to include in a concise, informative summary of text.  It does this by assigning
to each sentence in the document a probability which represents the probability
the sentence is used when creating the summary.

SummaRuNNer uses a hierarchy of models to make these probability predictions:

Word Level RNN:  
It begins by considering the words of each sentence individually.
It considers the words in their word vector format. Using a Bidirectional RNN with GRU’s
It creates word embeddings which represent the word along with the context gathered
from observing the surrounding words.  Then it aggregates all the word embeddings for
a sentence to find an average word embedding.

Sentence Level RNN:
The model uses another bidirectional RNN with GRU’s this time using the average word
embeddings as input.  What it outputs is a set of sentence embeddings.  These sentence
embeddings represent the content of sentence in addition to it’s context based on surrounding
sentences.

Document Embedding
A document embedding is created as  a nonlinear transformation of the average sentence
embeddings.

Nonlinear Sentence Embeddings
Nonlinear sentence embeddings are created by nonlinearly transforming the original sentence
embeddings

Probability Assignment:
5 factors are used in determining the probability of a sentence being selected for the
summary by SummaRuNNer.  Each sentence is considered in order.  At any given
sentence a previous summary embedding is available.  The summary embedding is
the summation of the previous nonlinear sentence embeddings and their respective
probabilities of being included in the summary.

1.Content Contribution: Measures the amount of information stored in a sentence.  
This is calculated by linearly transforming the nonlinear sentence embedding.

2.Salience Contribution: Measures the importance of the information stored in a
sentence considering the document as a whole.  It does this by performing a linear
transformation that considers both the nonlinear sentence embedding and the
document embedding.

3.Novelty Contribution: Measures the amount information stored in the sentence
not already captured previously while constructing the summary.  This is calculated
by performing a linear transformation which considers the nonlinear sentence
embedding and a nonlinear transformation of the current summary embedding.

4.Position Contribution:  Adds a component which considers the absolute position of
the sentence.  A scaled version of log(1+p) where p is the index of the sentence in the
document.

5.Bias:  A bias trained over time

The probability is the result of performing the sigmoid function on the summed contributions.

The generated summary is the concatenation of the sentences from the document for which
the assigned probability is greater than a binary threshold.

The loss is a log loss computed between the predicted probabilities and the ground truth labels.

-----(3) Technical Notes and Variations-----

Padding and Sentence Cap:
Due to the limitations created by the hierarchal sequential models at the Word and Sentence
Levels padding was necessary to allow the model to function.  Padding was done on the sentence
level.  This was to allow sentences to maintain variable length.  A number of sentences generated
by file_generator.py.  Documents of length less than NUM_SENTENCES are given ‘empty
sentences’ to pad up to NUM_SENTENCES.  Documents with more than NUM_SENTENCES are
truncated to NUM_SENTENCES.

Abstractive Training:
A method of abstractive training is mentioned in the research paper.  The idea is to have a separate
sequential model that goes from word embeddings to one hot representations of words in the hand
written summary.  Additionally the model is allowed to consider the earlier created summary
embedding.  This alternative method training is implemented in SummaRuNNer/old.  The reason
this alternate implementation wasn’t cleaned up and placed in the revised pipeline was due to 
suspicions that were confirmed during early experimentation.  The loss during training becomes
the error in the separate sequential model.  This does not directly measure the effectiveness of the
summary.  The intuition presented in the paper simply suggests that the model would likely
need to come up with a good summary embedding to facilitate this transition; however, this
doesn’t seem necessarily likely.  While this model performed worse than simple extractive training,
the implementation in SummaRuNNer/old may still be further experimented with.

Guaranteed Summary:
An addition personally made was the guarantee of at least one sentence being included in the
summary.  Ultimately the model depends on a binary threshold for determining which sentences
to include in the summary.  Sometimes it is the case that no sentence has a probability higher
then this threshold.  Since no summary is definitely the worst case, the sentence with maximum
probability is automatically given a 1 meaning that it is guaranteed to be in the summary.  This
avoids the issue of empty summaries.

-----(4) How to Use the Model-----

To run the script run model.py <data folder>.

At the very top of model.py there is a toggle ‘MODE’.  When MODE is set to 0 the script proceeds
to train the network with training data and provide live updates on performance on the validation
data.  When MODE is 1, the model makes predictions for documents in the validation and test
dataset and saves them to a file in outputs named predictions.json.  The predictions are in the
format of a dictionary mapping document ids to lists containing probabilities each sentence is
included in summary.  The appropriate document ids are determined using data_splits.

USE_LAST should be set to True if you want to use weights learned in the previous iteration and
False if you want to start from scratch.

NUM_DOCS_TRAIN is the number of docs to train with and NUM_DOCS_VAL is the number of
docs to provide live updates on.

Inside the train function LEARNING_RATE and EPOCHS can be changed as desired.

-----(5) How to Tune the Model-----

There hasn’t been significant hyperparameter optimization conducted at the moment, nonetheless
the following parameters may be easily changed if one wishes to optimize performance.

file_generator.py 

CAP_PERCENTILE can be changed to raise the sentence cap
or NUM_SENTENCES used in the model.  A higher CAP_PERCENTILE would make the model
more computationally intensive, and potentially reduce performance on the many documents
with early important sentences, but allow for predictions of later sentences in large documents.

WORD_VECTOR_SIZE represents the number of floats used in a word vector representation.  This
can be increased to allow the model to potentially develop more informative representations of 
words but at the same time make the model more computationally intensive.

model.py

NUM_WORD_UNITS represents number of floats used for word embeddings.

NUM_SENTENCE_UNITS represents number of floats used for sentence embeddings.

DOC_EMBEDDING_LENGTH represents number of floats used for document embedding.

All three of these hyperparameters can be increased to allow greater expressive power by
model.  The drawbacks include a more computationally expensive model and a higher 
chance of overfitting.

-----(6) Understanding the Model's Performance-----

Refer to evaluation.txt for some evaluation metrics and the png file for the ROC curve.
These files are within evaluate/SummaRuNNer

Some key takeaways from the model’s performance:

The recall of the model is similar to the recall of the labels.  What this means is that a 
close to optimal amount of information is being captured in the summary.  The precision 
values are significantly lower.  What this means is that the model is including some 
additional sentences that the labels figured out was unnecessary.  The Average Word 
Counts support the idea that the model is predicting longer summaries than either the 
label or the humans.  The model seems to be prioritizing getting all the information from 
the article over having the most concise summary which is a reasonable tradeoff to make.

For larger documents the model is incapable of including sentences beyond the sentence
cap in the summary.  Therefore the model will definitely perform worse when important 
information is included after sentence cap.  This is perhaps reflected by the lower recalls in 
larger documents displayed in DL_Comp. That is to say we aren’t getting all the important 
sentences as we aren’t considering sentences beyond the cap.

Since the model considers absolute position in its decision earlier sentences are favored as
opposed to later sentences.  This connects to the previous challenge of important information
being later in the document.

These performance metrics were generated after training the model on a small subset of the data.
Only a couple percent of the overall dataset was used in training.  With greater computational
resources it is likely that this model can achieve greater performance.