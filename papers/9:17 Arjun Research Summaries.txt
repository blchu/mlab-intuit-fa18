http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf


Summary:
        This 2013 paper discusses state of the art word representations using a network skip-gram model.  The goal is to learn word representations that are good at predicting neighbouring word and understanding context with an efficient algorithm.  Additionally we would like to be able to make logical combinations of words like “Madrid”- “Spain”+ “France” = “Paris.  The following algorithm meets both goals.
        The basic model to be improved upon is to first represent a word as a one-hot vector which of course will be a large vector.  Say there are v words in our vocabulary and we want an encoding of n numbers where n<<v.  Our hidden state matrix is of dimensions n x v.  Thus our hidden state is of n x 1: this can be thought of as the word encoding.  Then we multiply by a v x n matrix and apply a softmax to get a probability distribution for the next word.  One can intuitively think that each row of this matrix, each 1 x n vector is an “output” encoding of the word.  We may tab the word encoding of the input word as the “input” word encoding.  Word 2 is likely to occur after word 1 if Word 2’s “output” encoding is similar to Word 1’s “input encoding”.
        However there are multiple issues associated with this.  To begin with for any given word to determine the probability it predicted for the correct next word we have to compute the entire dense (v x n) x (n x1) matrix calculation for each word which can be a very costly computation as it doesn’t simply involve a one-hot vector as before.  This process may be computationally simplified through use of a hierarchical softmax layer.  The high level view of this is to rather than compute the softmax input for all words in the vocabulary utilize a binary tree approach.  Arrange all the words in the vocabulary randomly in a binary tree and remember the path to the desired word, the correct next word.  Then compute the probability that we take the correct branch of the tree at every juncture leading to the destination and taking the product of this.  This is an logarithmic operation as opposed to a linear operation.
        The second improvement is to use negative sampling.  Again negative sampling combats the issue of having a large vocabulary set.  The traditional skip-gram model involves decrementing the correlation between all wrong next words and the input word.  However, it turns out this process is actually not necessary and a better approach is to take a random sample of wrong next words and decrement their correlation.  Over time a similar effect is achieved and the process in combination with a hierarchical softmax saves a lot of time computationally.  For example say I had 100,000 words in my vocabulary.  A traditional softmax would compute the softmax inputs for all values and so it’s time complexity would be proportional to 100,000.  Now say instead we use use a hierarchical softmax which uses log_2(100,000) = 16.6 per word we need.  And say with negative sampling we choose to just use 10 negative samples.  This would overall be equal to (10+1)*16.6=183.  Thus this would run about 550 times faster than the traditional method.
        The third discussed improvement is of subsampling.  The idea here is that many words that occur frequently such as “the” don’t carry much meaning as opposed to words that occur less frequently like “basketball”.  Therefore we should alter the “input” or “output” vectors of less frequent words more than we do for frequent meaningless words.  This is achieved through subsampling.  When deciding which word pairs to train with during an Epoch discard pairs that involve more frequent words at a higher rate then those that don’t.  Additionally we impose a threshold which states that we won’t discard words that occur below a certain frequency.  This way we will spend more time training more important words and won’t dilute our network constantly training with common words that lack contextual information.
        One cool aspect of this algorithm is that since vector representations are computed linearly we can add them and subtract them and end up with meaningful results as mentioned earlier.  For example “German”+ “Airlines” = “Lufthansa” as the latter often occurs with the former.  Another example “Quick”- “Quickly” + “Slow” = “Slowly”.  These word vector representations capture both syntactic and semantic information of words.  Overall, the results are promising and these vector representations along with the improvements made have been shown to learn far quicker and make use of far more data.


https://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf


Summary:
        Data sparsity is a common NLP issue, as they are so many different possible combinations of words.  Consider the standard n-gram, that is n adjacent words in a text.  N-grams have proven to be a good way to model the context of a document as well as understand what words go together.  Suppose one has two text documents: one for training and one for testing.  Coverage is defined as the percentage of n-grams that appear in the test document that were present in the training document.
        Skip-grams are investigated as a method of improving coverage and reduce data sparsity without losing the context of a document.  A k skip n-gram is n words that appeared in order with at most k cumulative spaces in between adjacent words.  For example in the string “burritos are good for your health”  3 examples of a 2 skip trigram are “burritos good your”, “burritos are for”, and “good for your”.  Of course there are many more skip n-grams than normal n-grams, but we have to be careful as many skip-n grams are meaningless like “burritos good your”.
        Tests are conducted to see how skip grams improve coverage as well as maintain the context of the original document.  It is found that the coverage of all documents are increased using skip grams, but fortunately that of unrelated documents aren’t increased to the point where we lose the ability to differentiate between different contexts.  Skip n-grams are able to provide far greater coverage where more extensive data isn’t available.  This paper concludes that in certain situations skip n-grams are a suitable substitute for more data, due to their ability to generate more data and useful n-grams.


http://www.aclweb.org/anthology/J92-4003


Summary:
This rather dated research paper, discusses the use of classes in improving standard n-gram language models.  Generally speaking language models are created under the notion that there is a probability distribution for the next word conditioned on the previous sequence of words.  The standard n-gram model considers the previous n-1 words as it only looks at n words at a time.  The idea is that they make computation more feasible and the most relevant data should be closest to the word anyway.
A number of interesting results arise however when one sorts words into classes.  Instead of considering the specific word in the conditional probabilities one considers the class and the probability of the word amongst the class members.  The effects of having classes can be quantified through average mutual information.  Average mutual information is a measure of correlation between two words.  Specifically it’s is negative when it is unlikely that word 2 occurs after word 1 and positive for the opposite scenario.  A weighted average is generated amongst all possible combinations.
To create a class hierarchy one first assigns each words it’s own class. Then a greedy algorithm continues to merge classes that improve the prior mentioned metric the most.  This process continues until one has the desired amount of classes.  In doing so similar words are placed into similar classes granting the model a semantical understanding of many words.