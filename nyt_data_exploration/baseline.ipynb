{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mudit2103/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from statistics import mean\n",
    "import scipy.stats\n",
    "import nltk\n",
    "from nltk.tag import pos_tag \n",
    "\n",
    "nltk.download('stopwords')\n",
    "ENG_STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads in the following keys as variable names for the pickled files.\n",
    "pickle_files = {'abstract_sentences': \"AbstractSentences2007.pkl\", \n",
    "                'abstracts': \"Abstracts2007.pkl\",\n",
    "               'full_text_sentences': \"FullTextSentences2007.pkl\", \n",
    "                'full_text': \"FullTexts2007.pkl\",\n",
    "                'trained_vectors': \"trainedVectors2007.pkl\",\n",
    "                'labels': \"labels.pkl\"}\n",
    "\n",
    "for varname, p in pickle_files.items():\n",
    "    pickled_opened_file = open(p, \"rb\")\n",
    "    exec(varname + \" = pickle.load(pickled_opened_file)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_word(word):\n",
    "    \"\"\"Takes in a word and determines whether or not to keep it.\"\"\"\n",
    "    return word not in ENG_STOPWORDS and word != ',' and word != '.' and word != '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wc_map(full_text):\n",
    "    \"\"\"Given the full text data, returns a word-count map.\n",
    "    word_counts_map is a list, ordered by document_id. \n",
    "    Elements of the list are counter objects, which are a map\n",
    "    from the words in the document to the number of times a word\n",
    "    appears in corresponding document.\"\"\"\n",
    "    word_counts_map = []\n",
    "\n",
    "    for document in full_text:\n",
    "        cnt = Counter()\n",
    "        for word in document:\n",
    "            word = word.lower()\n",
    "            if keep_word(word):\n",
    "                cnt[word] += 1\n",
    "        word_counts_map.append(cnt)\n",
    "    return word_counts_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_sent_scores(full_text_sentences, word_counts_map):\n",
    "    \"\"\"document_sentence_scores is a list, indexed by document ID. \n",
    "    an element of the list is a map from sentence ID to sentence score.\n",
    "    Sentence score is defined as the $[sum( word frequency ) over all words in sentence]$\n",
    "    divided by the $num_words(sentence)$.\n",
    "    \n",
    "    Given full_text_sentences and word_counts_map, returns document_sentence_scores.\"\"\"\n",
    "    document_sentence_scores = []  \n",
    "    for i, document in enumerate(full_text_sentences):  # Iterate through all documents.\n",
    "        sentence_scores = {}  # Map from sentence_id to sentence_score\n",
    "        document_word_counts = word_counts_map[i]  # Counter object. Map from word->count(word) in this document.\n",
    "        num_doc_words = sum(document_word_counts.values())  # Total # of words in document.\n",
    "\n",
    "        for sentence_id, sentence in enumerate(document):  # Iterate through sentences in document.\n",
    "            sentence_word_freq_sum = 0\n",
    "            num_words_in_sentence = 0\n",
    "\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if keep_word(word):\n",
    "                    word_freq = document_word_counts[word] / num_doc_words  # count(word)/total_words(document)\n",
    "                    sentence_word_freq_sum += word_freq  \n",
    "                    num_words_in_sentence += 1  \n",
    "\n",
    "            # See defn of sentence_score in function comment.\n",
    "            sentence_score = sentence_word_freq_sum / num_words_in_sentence if num_words_in_sentence != 0 else 0\n",
    "            sentence_scores[sentence_id] = sentence_score\n",
    "\n",
    "        document_sentence_scores.append(sentence_scores)\n",
    "    return document_sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts_map = get_wc_map(full_text)\n",
    "document_sentence_scores = get_doc_sent_scores(full_text_sentences, word_counts_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_sentences(doc_id, N=5):\n",
    "    sentence_scores_map = document_sentence_scores[doc_id]\n",
    "    \n",
    "    sentence_scores = [(idx,score) for idx, score in sentence_scores_map.items()]\n",
    "    sentence_scores.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    sentence_scores = [(full_text_sentences[doc_id][idx], score) for idx, score in sentence_scores]\n",
    "    \n",
    "    return sentence_scores[:N]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_position(document_num, sentence_num, full_text_sentences):\n",
    "    \"\"\"Function which takes in the ID of the document, and the number of the sentence. \n",
    "    It returns the position of the sentence within the document, normalized.\n",
    "    This is to be used as one of the features in the featurized matrix for\n",
    "    logistic regression.\"\"\"\n",
    "    return sentence_num / len(full_text_sentences[document_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_length(document_num, sentence_num, full_text_sentences):\n",
    "    \"\"\"Function which takes in document ID, and the number of the sentence.\n",
    "    Returns the length of the sentence. \n",
    "    This function is written as such to match the header and style of the \n",
    "    other feature-functions, to allow us to easily use them in the feature matrix.\"\"\"\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proper_noun(document_num, sentence_num, full_text_sentences):\n",
    "    \"\"\"Function which takes in document ID, and the number of the sentence. \n",
    "    Returns the number of proper nouns within sentence. \n",
    "    Uses nltk's tagging to determine which words are proper nouns.\"\"\"\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    tagged_sent = pos_tag(sentence)\n",
    "    propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
    "    \n",
    "    return len(propernouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_freq_score(document_num, sentence_num, full_text_sentences):\n",
    "    \"\"\"Function which takes in document ID, and the number of the sentence. \n",
    "    Returns document_sentence_score for the corresponding document and sentence.\"\"\"\n",
    "    return 1000 * document_sentence_scores[document_num][sentence_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of feature-functions, to allow us to easily create the feature matrix.\n",
    "features_functions = [sentence_position, sentence_length, proper_noun, sentence_freq_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Indices of files used to generate the labels. \n",
    "# Note that not all files were used, since some of them were formatted poorly. \n",
    "# The first element of 'labels' contains the indices of the file/document numbers.\n",
    "file_numbers = [x[0] for x in labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "def create_ft_matrix(fts, test=10):\n",
    "    \"\"\"Returns the featurized matrix, given the full_text_sentences\"\"\"\n",
    "    X = np.zeros((1, 4))  # Feature matrix. Initialize as a row of zeros.\n",
    "    \n",
    "    end = test if test else len(file_numbers)  # Test can be a number for using a subset of documents, else False.\n",
    "    for i in file_numbers[:end]: \n",
    "        document = full_text_sentences[i]\n",
    "        for j, sentence in enumerate(document):\n",
    "            X = np.vstack([X, [function(i, j, fts) for function in features_functions]])\n",
    "    X = X[1:]  # Remove the initial row of zeros.\n",
    "    return X  # Return the feature matrix.\n",
    "\n",
    "def get_corr_labels(labels, test=10):\n",
    "    \"\"\"Given all the labels information in a list of (file number, actual-labels) tuples, \n",
    "    returns  labels (b vector) corresponding to the feature matrix.\n",
    "    \"\"\"\n",
    "    end = test if test else len(labels)\n",
    "    corr_labels = [x[1] for x in labels[:end]]\n",
    "    return corr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = create_ft_matrix(number=1000)\n",
    "corr_labels = get_corr_labels(number=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38841, 4)\n",
      "38841\n"
     ]
    }
   ],
   "source": [
    "flat_list = []\n",
    "for sublist in corr_labels:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "print(X.shape)\n",
    "print(len(flat_list))\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8143785801634807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = flat_list\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "threshold = 0.5\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "predictions = [1 if x[1] > 0.15 else 0 for x in probabilities]\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23304, 4) (15537, 4) 23304 15537\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from pythonrouge.pythonrouge import Pythonrouge\n",
    "\n",
    "def bleu(lst, target_sentence, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "\tsent_scores_map = dict()\n",
    "\tmax_score = None\n",
    "\tbest_sentence = None\n",
    "\tbest_idx = None\n",
    "\t\n",
    "\tfor i, sentence in enumerate(lst):\n",
    "\t    sentence = [sentence]\n",
    "\t    score = sentence_bleu(sentence, target_sentence, weights)\n",
    "\t    sent_scores_map[i] = score\n",
    "\t    if max_score is None or score > max_score:\n",
    "\t        max_score = score\n",
    "\t        best_sentence = sentence\n",
    "\t        best_idx = i\n",
    "\treturn best_idx, sent_scores_map\n",
    "\n",
    "def untokenize(lst):\n",
    "\tuntokenized = []\n",
    "\tfor item in lst:\n",
    "\t\tsentence = ' '.join(item)\n",
    "\t\tuntokenized.append([sentence])\n",
    "\treturn untokenized\n",
    "\n",
    "def rouge(lst, target_sentence, weights=None):\n",
    "\t# The weights parameter is currently ignored\n",
    "\tuntokenized_list = untokenize(lst) # list of lists\n",
    "\ttarget_sentence = [target_sentence]\n",
    "\n",
    "\tsent_scores_map = dict()\n",
    "\tmax_score = None\n",
    "\tbest_sentence = None\n",
    "\tbest_idx = None\n",
    "\n",
    "\tfor i, sentence in enumerate(untokenized_list):\n",
    "\t\tsentence = [[sentence]]\n",
    "\t\trouge = Pythonrouge(summary_file_exist=False,\n",
    "                    summary=target_sentence, reference=sentence,\n",
    "                    n_gram=2, ROUGE_SU4=True, ROUGE_L=False,\n",
    "                    recall_only=True, stemming=True, stopwords=True,\n",
    "                    word_level=True, length_limit=True, length=50,\n",
    "                    use_cf=False, cf=95, scoring_formula='average',\n",
    "                    resampling=True, samples=1000, favor=True, p=0.5)\n",
    "\t\tscore = rouge.calc_score()['ROUGE-1']\n",
    "\t\tsent_scores_map[i] = score\n",
    "\t\tif max_score is None or score > max_score:\n",
    "\t\t\tmax_score = score\n",
    "\t\t\tbest_sentence = sentence\n",
    "\t\t\tbest_idx = i\n",
    "\n",
    "\treturn best_idx, sent_scores_map\n",
    "\n",
    "def f1_score(bleu_score, rouge_score):\n",
    "\treturn float((2*(bleu_score*rouge_score))) / float((bleu_score + rouge_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {0: 1.0, 1: 0.7071067811865475})\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# print(\"Bleu: \", bleu(list(predictions), y_test))\n",
    "\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "\n",
    "print(bleu(reference, candidate))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3103\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "print(len([p for p in predictions if p == 1]))\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summaries = []\n",
    "from functools import reduce\n",
    "def document_indices(doc_id):\n",
    "    prev_docs = full_text_sentences[0:doc_id]\n",
    "    prev_docs_length = sum([len(x) for x in prev_docs])\n",
    "    doc = full_text_sentences[doc_id]\n",
    "    doc_len = len(doc)\n",
    "    \n",
    "    start = prev_docs_length\n",
    "    end = start + doc_len\n",
    "    \n",
    "    return start, end\n",
    "\n",
    "def flatten(lst):\n",
    "    flattened_list = []\n",
    "    for sublist in lst:\n",
    "        for item in sublist:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list\n",
    "\n",
    "def average(scores):\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def get_average_scores():\n",
    "    bleu_logistic_scores = []\n",
    "    bleu_label_scores = []\n",
    "    rouge_logistic_scores = []\n",
    "    rouge_label_scores = []\n",
    "    for k, i in enumerate(file_numbers[:1000]):\n",
    "        flattened_abstract_sentences = flatten(abstract_sentences[i])\n",
    "    #     summaries.append(flattened_abstract_sentences)\n",
    "    #     print(flattened_abstract_sentences)\n",
    "        reference = [flattened_abstract_sentences]\n",
    "        start, end = document_indices(i)\n",
    "        candidate_logistic = flatten([full_text_sentences[i][index] for index in predictions[start:end]])\n",
    "        candidate_labels = flatten([full_text_sentences[i][index] for index in labels[k][1]])\n",
    "        best_idx, sent_map = bleu(reference, candidate_logistic)\n",
    "        bleu_logistic_scores.append(sent_map[0])\n",
    "        best_idx, sent_map = bleu(reference, candidate_labels)\n",
    "        bleu_label_scores.append(sent_map[0])\n",
    "        \n",
    "#         best_idx, sent_map = rouge(reference, candidate_logistic)\n",
    "#         rouge_label_scores.append(sent_map[0])\n",
    "#         best_idx, sent_map = rouge(reference, candidate_labels)\n",
    "#         rouge_label_scores.append(sent_map[0])\n",
    "\n",
    "    return average(bleu_logistic_scores), average(bleu_label_scores)#, average(rouge_logistic_scores), average(rouge_label_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/anaconda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/anaconda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.073473928921661, 0.1785201062177081)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
