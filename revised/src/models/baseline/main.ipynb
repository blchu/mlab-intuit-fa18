{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import scipy.stats\n",
    "import nltk\n",
    "from nltk.tag import pos_tag \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "DATA_DIR = \"../../../data/\"\n",
    "OUTPUT_DIR = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughout the rest of the code, it is assumed that the keys of the following dictionary\n",
    "# are global variables, available to freely use. These global variables are modified only in this block. \n",
    "json_files = {'abstract_sentences': \"abstracts.json\", \n",
    "                'full_text_sentences': \"fulltexts.json\", \n",
    "                'labels': \"labels.json\",\n",
    "                'data_splits': \"data_splits.json\"}   ## New Preprocess\n",
    "              #'train_indices': 'train_indices.json',\n",
    "                #'test_indices': 'test_indices.json'}\n",
    "\n",
    "for varname, j in json_files.items():\n",
    "    qualified_name = DATA_DIR + j\n",
    "    file = open(qualified_name, \"rb\")\n",
    "    exec(varname + \" = json.load(file)\")\n",
    "    \n",
    "relevant_file_numbers = [x[0] for x in labels]\n",
    "\n",
    "## New Preprocess\n",
    "#full_text_sentences = np.array(full_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_word(word):\n",
    "    return word not in eng_stopwords and word != ',' and word != '.' and word != '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wc_map(full_text_sentences):\n",
    "    word_counts_map = []\n",
    "    for document in full_text_sentences.values():\n",
    "    #for document in full_text_sentences: #Modified\n",
    "        cnt = Counter()\n",
    "        for sentence in document:\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if keep_word(word):\n",
    "                    cnt[word] += 1\n",
    "            word_counts_map.append(cnt)\n",
    "    return word_counts_map\n",
    "\n",
    "word_counts_map = get_wc_map(full_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_scores(full_text_sentences, word_counts_map):\n",
    "    document_sentence_scores = {} # Map of maps from sentence_id to score.\n",
    "    #document_sentence_scores = [] # List of maps from sentence_id to score. #Modified\n",
    "    \n",
    "    for i,(document_id, document) in enumerate(full_text_sentences.items()): #Modified\n",
    "        sentence_scores = {} # Map for this document.\n",
    "        document_word_counts = word_counts_map[i] \n",
    "        num_doc_words = sum(document_word_counts.values())\n",
    "\n",
    "        for sentence_id, sentence in enumerate(document):\n",
    "            sentence_word_freq_sum = 0\n",
    "            num_words_in_sentence = 0\n",
    "\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if keep_word(word):\n",
    "                    word_freq = document_word_counts[word] / num_doc_words\n",
    "                    sentence_word_freq_sum += word_freq\n",
    "                    num_words_in_sentence += 1\n",
    "\n",
    "            sentence_score = sentence_word_freq_sum / num_words_in_sentence if num_words_in_sentence != 0 else 0\n",
    "            sentence_scores[sentence_id] = sentence_score \n",
    "        document_sentence_scores[document_id] = sentence_scores\n",
    "        #document_sentence_scores.append(sentence_scores) #Modified\n",
    "    return document_sentence_scores\n",
    "\n",
    "document_sentence_scores = get_doc_scores(full_text_sentences, word_counts_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_position(document_num, sentence_num):\n",
    "    return sentence_num / len(full_text_sentences[document_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(document_num, sentence_num, mean_sent_length=None, std_dev=5):\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_noun(document_num, sentence_num):\n",
    "    sentence = full_text_sentences[document_num][sentence_num]\n",
    "    tagged_sent = pos_tag(sentence)\n",
    "    propernouns = [word for word, pos in tagged_sent if pos == 'NNP']    \n",
    "    return len(propernouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_freq_score(document_num, sentence_num):\n",
    "    score = 1000 * document_sentence_scores[document_num][sentence_num]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_functions = [sentence_position, sentence_length, proper_noun, sentence_freq_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1670"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'val', 'test', 'val_subset', 'train_subset'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use test in the end\n",
    "# 80, 10, 10\n",
    "\n",
    "file_numbers = [x[0] for x in labels]\n",
    "\n",
    "## New Preprocess\n",
    "train_indices = data_splits['train']\n",
    "val_indices = data_splits['val']\n",
    "test_indices = data_splits['test']\n",
    "\n",
    "# Make sure these are regular lists and not numpy arrays. Things will break if they are numpy arrays.\n",
    "assert type(train_indices) == list\n",
    "assert type(test_indices) == list\n",
    "\n",
    "# print(train_indices)\n",
    "# print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "def create_ft_matrix(test=True, number=10):\n",
    "    # Creates an X matrix with the train index file numbers' sentences appearing FIRST<\n",
    "    # followed by test index file numbers' sentences.\n",
    "    X = np.zeros((1, 4))\n",
    "    for i in train_indices[:100] + test_indices + val_indices:\n",
    "        document = full_text_sentences[i]\n",
    "        for j, sentence in enumerate(document):\n",
    "            X = np.vstack([X, [function(i, j) for function in features_functions]])\n",
    "    X = X[1:]\n",
    "    return X\n",
    "\n",
    "def get_corr_labels(test=True, number=10):\n",
    "    relevant_file_numbers = train_indices[:100] + test_indices + val_indices\n",
    "    corr_labels = []\n",
    "    \n",
    "    for file_num, labels_list in labels.items(): #Modified\n",
    "        if file_num in relevant_file_numbers:\n",
    "            corr_labels.append(labels_list)\n",
    "    return corr_labels\n",
    "\n",
    "def get_num_sentences(file_numbers):\n",
    "    total_num_sentences = 0\n",
    "    for i in file_numbers:\n",
    "        sentences = full_text_sentences[i]\n",
    "        total_num_sentences += len(sentences) \n",
    "    return total_num_sentences\n",
    "        \n",
    "\n",
    "def flatten(lst):\n",
    "    flattened_list = []\n",
    "    for sublist in lst:\n",
    "        for item in sublist:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_ft_matrix() # Took ~1 hr 6:03~6:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_labels = flatten(get_corr_labels()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = corr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134265, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == len(corr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9466406801510082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Number of sentences (i.e. rows) to use for training.\n",
    "nts = get_num_sentences(train_indices[:100])\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[0:nts], X[nts:], y[0:nts], y[nts:]\n",
    "\n",
    "threshold = 0.15\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "predictions = [1 if x[1] > threshold else 0 for x in probabilities] \n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3941, 4) (130324, 4) 3941 130324\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = [3,4]\n",
    "print(a+b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(probabilities) == get_num_sentences(test_indices+val_indices) # Must have a probability for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(probabilities):\n",
    "    output = dict()\n",
    "    \n",
    "    used_so_far = 0\n",
    "    for test_index in test_indices+val_indices:\n",
    "        doc = full_text_sentences[test_index]\n",
    "        ns = len(doc) # number of sentences in this document\n",
    "        \n",
    "        output[test_index] = probabilities[used_so_far : used_so_far + ns].tolist()\n",
    "        used_so_far += ns\n",
    "    return output\n",
    "\n",
    "output = generate_output(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(output):\n",
    "    json.dump(output, open(OUTPUT_DIR + \"predictions.json\", 'w'))\n",
    "\n",
    "save_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
